{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLVMM-20NewsGroup.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HPQssjNK9ZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "18eff6ae-15a2-4f40-aa34-a22bb65a91d2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frfmrtLFLGtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "868edeed-d2c5-499f-98cc-7d493818a0f4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',max_features=2000)\n",
        "trainX = vectorizer.fit_transform(newsgroups_train.data)\n",
        "testX = vectorizer.transform(newsgroups_test.data)\n",
        "voc = vectorizer.vocabulary_\n",
        "print (trainX.shape)\n",
        "testX.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(11314, 2000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7532, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoxgq9IkLJdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "250b38fb-4957-4ad9-fbcf-f3957ce4291d"
      },
      "source": [
        "temp = 0.1\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,2000])\n",
        "x = tf.placeholder(tf.float32, shape=[None,2000])\n",
        "\n",
        "batch = tf.placeholder(tf.int32, shape=())\n",
        "\n",
        "#WH = tf.Variable(tf.zeros([120,90]),name='WH')\n",
        "#bH = tf.Variable(tf.zeros([90]),name='biasH')\n",
        "\n",
        "def decoder(x,name=\"classDecoder\"):\n",
        "\n",
        "    #with tf.variable_scope(name):\n",
        "\n",
        "        W1 = tf.Variable(tf.random.uniform([2000,300],-0.5,0.5),name='W1',dtype=tf.float32)\n",
        "        b1 = tf.Variable(tf.zeros([300]),name='bias1',dtype=tf.float32)\n",
        "\n",
        "        layer1 = tf.nn.sigmoid(tf.matmul(x,W1) + b1)\n",
        "\n",
        "        W2 = tf.Variable(tf.random.uniform([300,250],-0.5,0.5),name='W2')\n",
        "        b2 = tf.Variable(tf.zeros([250]),name='bias2',dtype=tf.float32)\n",
        "\n",
        "        layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n",
        "\n",
        "        W3 = tf.Variable(tf.random.uniform([250,50],-0.05,0.05),name='WMu')\n",
        "        b3 = tf.Variable(tf.zeros([50]),name='biasMu',dtype=tf.float32)\n",
        "\n",
        "        W4 = tf.Variable(tf.random.uniform([250,50],0,0),name='WSigma')\n",
        "        b4 = tf.Variable(tf.zeros([50]),name='biasSigma',dtype=tf.float32)\n",
        "\n",
        "        mu = tf.matmul(layer2,W3) + b3\n",
        "        log_sigma = tf.matmul(layer2,W4) + b4\n",
        "\n",
        "        return mu,log_sigma\n",
        "\n",
        "def decoder2(hs,name=\"C\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "    \n",
        "        W1 = tf.Variable(tf.random.normal([2000,300],-0.05,0.05),name='W1')\n",
        "        b1 = tf.Variable(tf.zeros([300]),name='bias1',dtype=tf.float32)\n",
        "\n",
        "        \n",
        "        layer1 = tf.nn.sigmoid(tf.matmul(hs,W1) + b1)\n",
        "\n",
        "        W2 = tf.Variable(tf.random.uniform([300,150],-0.05,0.05),name='W2')\n",
        "        b2 = tf.Variable(tf.zeros([150]),name='bias2',dtype=tf.float32)\n",
        "\n",
        "        layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
        "\n",
        "        W5 = tf.Variable(tf.random.uniform([150,4],-0.00,0.00),name='WC')\n",
        "        b5 = tf.Variable(tf.zeros([4]),name='biasC',dtype=tf.float32)\n",
        "        \n",
        "\n",
        "        C = tf.nn.softmax(tf.sigmoid(tf.matmul(layer2,W5) + b5))\n",
        "\n",
        "        return C\n",
        "\n",
        "\n",
        "def encoder(x,h,name='ClassEncoder'):\n",
        "\n",
        "     with tf.variable_scope(name):\n",
        "\n",
        "          R = tf.Variable(tf.random.uniform([50,2000],-0.01,0.01),name='RWord')\n",
        "          b = tf.Variable(tf.zeros([2000]),name='biasWord')\n",
        "\n",
        "          probs = tf.log(tf.nn.softmax(tf.matmul(h,R)+b))\n",
        "\n",
        "          final = tf.reduce_sum(tf.multiply(probs,x),axis=1)\n",
        "\n",
        "          return -final,R\n",
        "\n",
        "\n",
        "networks = []\n",
        "networks2 = []\n",
        "ss = []\n",
        "df = []\n",
        "pi = tf.distributions.Categorical(probs=[0.25,0.25,0.25,0.25])\n",
        "mu,log_sigma = decoder(x)\n",
        "eps = tf.random_normal((batch,50), 0,1)\n",
        "h = mu+tf.multiply(tf.exp(log_sigma),eps)\n",
        "rtt = []\n",
        "\n",
        "for l in range(0,4):\n",
        "\n",
        "\n",
        "    probsE,goes = encoder(x,h,name=\"Class_\"+str(l))\n",
        "    networks.append(probsE)\n",
        "    df.append(h)\n",
        "    rtt.append(goes)\n",
        "    #networks2.append(-0.5 * tf.reduce_sum(1 - tf.square(mu) + 2 * log_sigma - tf.exp(2 * log_sigma), 1))\n",
        "\n",
        "\n",
        "C = decoder2(x)\n",
        "eps2 = tf.random_uniform((batch,4), 0,1)\n",
        "eps2 = -tf.log(-tf.log(eps2))\n",
        "\n",
        "C2 = tf.log(C)+eps2\n",
        "C2 = tf.nn.softmax(C2/temp,axis=1)\n",
        "\n",
        "enc2 = networks*tf.transpose(C2)\n",
        "enc = tf.reduce_sum(enc2,axis=0)\n",
        "\n",
        "\n",
        "kld = -tf.reduce_sum(pi.probs*tf.log(pi.probs/C),axis=1)\n",
        "kld2 = -0.5 * tf.reduce_sum(1 - tf.square(mu) + 2 * log_sigma - tf.exp(2 * log_sigma), 1) \n",
        "train = enc\n",
        "train1 = train + kld + kld2\n",
        "train2 = tf.reduce_mean(train1)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0013)\n",
        "\n",
        "step = optimizer.minimize(train2)\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "print('Initializing...')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "#saver.save(sess,path+'first_save')\n",
        "#feed_dict = {x:trainX[0].reshape(1,2000)}\n",
        "#sess.run(step,feed_dict=feed_dict)\n",
        "#sess.run(train,feed_dict=feed_dict)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-c1f0d32a4507>:78: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/distributions/categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "Initializing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5HzwEJhLP1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e890a171-bd48-4fd3-8318-c39d5bc55475"
      },
      "source": [
        "from random import shuffle\n",
        "indices = np.arange(trainX.shape[0]) #gets the number of rows \n",
        "shuffle(indices)\n",
        "print (indices)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2273 10444  1566 ...  4967   926  2073]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZiDC0zTLSgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "39925c0f-1b71-401d-cb75-7251ed125e41"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "for ll in range(0,25):\n",
        "\n",
        "    #print (begin,end,\"<===>\")\n",
        "    begin = 0\n",
        "    end = 32\n",
        "    cont = 0\n",
        "    while end <=11314:\n",
        "\n",
        "        select = indices[begin:end]\n",
        "        feed_dict = {x:trainX[select].toarray().reshape(32,2000),batch:32}\n",
        "        sess.run(step,feed_dict=feed_dict)\n",
        "\n",
        "        begin = end\n",
        "        end += 32\n",
        "            \n",
        "        cont +=1\n",
        "\n",
        "    print (\"########\")\n",
        "    feed_dict = {x:testX.toarray().reshape(7532,2000),batch:7532}\n",
        "    a = sess.run(train1,feed_dict=feed_dict)\n",
        "    ppr  = np.exp(np.sum(a/np.sum(testX.toarray(),axis=1))/7532.0)\n",
        "    print (ppr)\n",
        "    #1291"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########\n",
            "1180.9269970905577\n",
            "########\n",
            "1101.2138906030611\n",
            "########\n",
            "1060.1107094710405\n",
            "########\n",
            "1020.7050474951199\n",
            "########\n",
            "997.660721193846\n",
            "########\n",
            "973.1300430439517\n",
            "########\n",
            "947.8046660994715\n",
            "########\n",
            "929.3844054233617\n",
            "########\n",
            "911.8034418327793\n",
            "########\n",
            "893.4661076489798\n",
            "########\n",
            "881.324936427153\n",
            "########\n",
            "867.6051977156646\n",
            "########\n",
            "858.6720843067258\n",
            "########\n",
            "846.5714556140106\n",
            "########\n",
            "834.90579640781\n",
            "########\n",
            "820.9018981056438\n",
            "########\n",
            "815.8885983913359\n",
            "########\n",
            "808.2972204705154\n",
            "########\n",
            "799.2108790308259\n",
            "########\n",
            "793.1731944434033\n",
            "########\n",
            "782.6005098094555\n",
            "########\n",
            "774.2380411109897\n",
            "########\n",
            "772.3034652540958\n",
            "########\n",
            "760.0584932134313\n",
            "########\n",
            "755.4571657107845\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}